{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_V6_2_128px_without_cometml.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc0c05a05ff84229a6ab8eb32783e09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_465a6f8aff6a4924aff55f808536a0b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c88273c3c0af4485aa796ecacfc5c21d",
              "IPY_MODEL_cca4bbb1f9bb49f082c74edb50c29fc9"
            ]
          }
        },
        "465a6f8aff6a4924aff55f808536a0b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c88273c3c0af4485aa796ecacfc5c21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2b295e333c848ccb33feb8bda6cd7a2",
            "_dom_classes": [],
            "description": "Epoch 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_524796e3ee2b47e881d6c435a244a727"
          }
        },
        "cca4bbb1f9bb49f082c74edb50c29fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c29b6f0a22ed41809d110459ef585b61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  2.42it/s, d_loss=0.865, g_loss=0.884, loss=0.875, v_num=6]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a58e2fccb6e49c193caea3f88236bc9"
          }
        },
        "e2b295e333c848ccb33feb8bda6cd7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "524796e3ee2b47e881d6c435a244a727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c29b6f0a22ed41809d110459ef585b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a58e2fccb6e49c193caea3f88236bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTp7EUU_kT1x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "807ee578-43ac-4dec-cdff-4d3720914b9e"
      },
      "source": [
        "!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch===1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision===0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision===0.5.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision===0.5.0) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision===0.5.0) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL2WMyUhfkyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "8e920dd2-14bd-45c0-a943-e8939ad89fd6"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (0.7.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.45.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.34.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (46.1.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.28.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.6.0.post3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.9.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (2.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aOFSycYfmFf"
      },
      "source": [
        "# used fo TPU\n",
        "# Uncomment if you want to use TPU\n",
        "# ------------------------------------------------------------------\n",
        "# import collections\n",
        "# from datetime import datetime, timedelta\n",
        "# import os\n",
        "# import requests\n",
        "# import threading\n",
        "\n",
        "# _VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "# VERSION = \"xrt==1.15.0\"  # @param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "# CONFIG = {\n",
        "#     'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "#     'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "#         (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "# }[VERSION]\n",
        "# DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "# TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "# TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "# TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# # Update TPU XRT version\n",
        "# def update_server_xrt():\n",
        "#     print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "#     url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "#         TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "#         XRT_VERSION=CONFIG.server,\n",
        "#     )\n",
        "#     print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "\n",
        "# update = threading.Thread(target=update_server_xrt)\n",
        "# update.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHLEBnnSfosm"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers\n",
        "\n",
        "from numpy.random import choice\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjjEWS1Lfthl"
      },
      "source": [
        "# lets hope this fixes the bug that my pc crashes after reconnecting\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmTaKo7mfwxc"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx70EX9wfx1D"
      },
      "source": [
        "# randomly flip some labels\n",
        "def noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n",
        "    # determine the number of labels to flip\n",
        "    n_select = int(p_flip * y.shape[0])\n",
        "    # choose labels to flip\n",
        "    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n",
        "    # invert the labels in place\n",
        "    y[flip_ix] = 1 - y[flip_ix]\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noCnaCJZfzHm"
      },
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0.0, std=0.1):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        tensor = tensor.cuda()\n",
        "        return tensor + (torch.randn(tensor.size()) * self.std + self.mean).cuda()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzAjUIW5f0kd"
      },
      "source": [
        "def resize2d(img, size):\n",
        "    return (F.adaptive_avg_pool2d(img, size).data).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EptHLAyIf3xB"
      },
      "source": [
        "def get_valid_labels(img):\n",
        "    return ((0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1).cuda()  # soft labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v-p-xCFf5HG"
      },
      "source": [
        "def get_unvalid_labels(img):\n",
        "    return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3).cuda()  # soft labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2amoqoiFf6WO"
      },
      "source": [
        "class Generator(pl.LightningModule):\n",
        "    def __init__(self, ngf, nc, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngf = ngf\n",
        "        self.latent_dim = latent_dim\n",
        "        self.nc = nc\n",
        "\n",
        "        self.fc0 = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(latent_dim, ngf * 16, 4, 1, 0, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ngf * 16)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ngf * 8)\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ngf * 4)\n",
        "        )\n",
        "\n",
        "        self.fc3 = nn.Sequential(\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ngf * 2)\n",
        "        )\n",
        "\n",
        "        self.fc4 = nn.Sequential(\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ngf)\n",
        "        )\n",
        "\n",
        "        self.fc5 = nn.Sequential(\n",
        "            # state size. (nc) x 64 x 64\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # state size. (nc) x 128 x 128\n",
        "\n",
        "        # For Multi-Scale Gradient\n",
        "        # Converting the intermediate layers into images\n",
        "        self.fc0_r = nn.Conv2d(ngf * 16, self.nc, 1)\n",
        "        self.fc1_r = nn.Conv2d(ngf * 8, self.nc, 1)\n",
        "        self.fc2_r = nn.Conv2d(ngf * 4, self.nc, 1)\n",
        "        self.fc3_r = nn.Conv2d(ngf * 2, self.nc, 1)\n",
        "        self.fc4_r = nn.Conv2d(ngf, self.nc, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x_0 = self.fc0(input)\n",
        "        x_1 = self.fc1(x_0)\n",
        "        x_2 = self.fc2(x_1)\n",
        "        x_3 = self.fc3(x_2)\n",
        "        x_4 = self.fc4(x_3)\n",
        "        x_5 = self.fc5(x_4)\n",
        "\n",
        "        # For Multi-Scale Gradient\n",
        "        # Converting the intermediate layers into images\n",
        "        x_0_r = self.fc0_r(x_0)\n",
        "        x_1_r = self.fc1_r(x_1)\n",
        "        x_2_r = self.fc2_r(x_2)\n",
        "        x_3_r = self.fc3_r(x_3)\n",
        "        x_4_r = self.fc4_r(x_4)\n",
        "\n",
        "        return x_5, x_0_r, x_1_r, x_2_r, x_3_r, x_4_r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PFMmdgzf8Xx"
      },
      "source": [
        "class Discriminator(pl.LightningModule):\n",
        "    def __init__(self, ndf, nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.ndf = ndf\n",
        "\n",
        "        self.fc0 = nn.Sequential(\n",
        "            # input is (nc) x 128 x 128\n",
        "            nn.utils.spectral_norm(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            # state size. (ndf) x 64 x 64\n",
        "            nn.utils.spectral_norm(nn.Conv2d(ndf + nc, ndf * 2, 4, 2, 1, bias=False)),\n",
        "            # \"+ nc\" because of multi scale gradient\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ndf * 2)\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            # state size. (ndf*2) x 32 x 32\n",
        "            nn.utils.spectral_norm(nn.Conv2d(ndf * 2 + nc, ndf * 4, 4, 2, 1, bias=False)),\n",
        "            # \"+ nc\" because of multi scale gradient\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ndf * 4)\n",
        "        )\n",
        "\n",
        "        self.fc3 = nn.Sequential(\n",
        "            # state size. (ndf*4) x 16 x 16e\n",
        "            nn.utils.spectral_norm(nn.Conv2d(ndf * 4 + nc, ndf * 8, 4, 2, 1, bias=False)),\n",
        "            # \"+ nc\" because of multi scale gradient\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "        )\n",
        "\n",
        "        self.fc4 = nn.Sequential(\n",
        "            # state size. (ndf*8) x 8 x 8\n",
        "            nn.utils.spectral_norm(nn.Conv2d(ndf * 8 + nc, ndf * 16, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(ndf * 16)\n",
        "        )\n",
        "\n",
        "        self.fc5 = nn.Sequential(\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.utils.spectral_norm(nn.Conv2d(ndf * 16 + nc, 1, 4, 1, 0, bias=False)),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # state size. 1 x 1 x 1\n",
        "\n",
        "    def forward(self, input, detach_or_not):\n",
        "        # When we train i ncombination with generator we use multi scale gradient.\n",
        "        x, x_0_r, x_1_r, x_2_r, x_3_r, x_4_r = input\n",
        "        if detach_or_not:\n",
        "            x = x.detach()\n",
        "\n",
        "        x_0 = self.fc0(x)\n",
        "\n",
        "        x_0 = torch.cat((x_0, x_4_r), dim=1)  # Concat Multi-Scale Gradient\n",
        "        x_1 = self.fc1(x_0)\n",
        "\n",
        "        x_1 = torch.cat((x_1, x_3_r), dim=1)  # Concat Multi-Scale Gradient\n",
        "        x_2 = self.fc2(x_1)\n",
        "\n",
        "        x_2 = torch.cat((x_2, x_2_r), dim=1)  # Concat Multi-Scale Gradient\n",
        "        x_3 = self.fc3(x_2)\n",
        "\n",
        "        x_3 = torch.cat((x_3, x_1_r), dim=1)  # Concat Multi-Scale Gradient\n",
        "        x_4 = self.fc4(x_3)\n",
        "\n",
        "        x_4 = torch.cat((x_4, x_0_r), dim=1)  # Concat Multi-Scale Gradient\n",
        "        x_5 = self.fc5(x_4)\n",
        "\n",
        "        return x_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPAhNKVf-oF"
      },
      "source": [
        "class DCGAN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, hparams, checkpoint_folder, experiment_name):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.checkpoint_folder = checkpoint_folder\n",
        "        self.experiment_name = experiment_name\n",
        "\n",
        "        # networks\n",
        "        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n",
        "        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n",
        "        self.generator.apply(weights_init)\n",
        "        self.discriminator.apply(weights_init)\n",
        "\n",
        "        # cache for generated images\n",
        "        self.generated_imgs = None\n",
        "        self.last_imgs = None\n",
        "\n",
        "        # For experience replay\n",
        "        self.exp_replay_dis = torch.tensor([])\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def adversarial_loss(self, y_hat, y):\n",
        "        return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "    def training_step(self, batch, batch_nb, optimizer_idx):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # For adding Instance noise for more visit: https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/\n",
        "        std_gaussian = max(0, self.hparams.level_of_noise - (\n",
        "                (self.hparams.level_of_noise * 2) * (self.current_epoch / self.hparams.epochs)))\n",
        "        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian)  # the noise decays over time\n",
        "\n",
        "        imgs, _ = batch\n",
        "        imgs = AddGaussianNoiseInst(imgs)  # Adding instance noise to real images\n",
        "        self.last_imgs = imgs\n",
        "\n",
        "        # train generator\n",
        "        if optimizer_idx == 0:\n",
        "            # sample noise\n",
        "            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1).cuda()\n",
        "\n",
        "            # generate images\n",
        "            self.generated_imgs = self(z)\n",
        "\n",
        "            # ground truth result (ie: all fake)\n",
        "            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs, False), get_valid_labels(\n",
        "                self.generated_imgs[0]))  # adversarial loss is binary cross-entropy; [0] is the image of the last layer\n",
        "\n",
        "            tqdm_dict = {'g_loss': g_loss}\n",
        "            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n",
        "            output = OrderedDict({\n",
        "                'loss': g_loss,\n",
        "                'progress_bar': tqdm_dict,\n",
        "                'log': log\n",
        "            })\n",
        "            return output\n",
        "\n",
        "        # train discriminator\n",
        "        if optimizer_idx == 1:\n",
        "            # Measure discriminator's ability to classify real from generated samples\n",
        "            # how well can it label as real?\n",
        "            real_loss = self.adversarial_loss(\n",
        "                self.discriminator([imgs, resize2d(imgs, 4), resize2d(imgs, 8), resize2d(imgs, 16), resize2d(imgs, 32), resize2d(imgs, 64)],\n",
        "                                   False), get_valid_labels(imgs))\n",
        "\n",
        "            fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs, True), get_unvalid_labels(\n",
        "                self.generated_imgs[0]))  # how well can it label as fake?; [0] is the image of the last layer\n",
        "\n",
        "            # discriminator loss is the average of these\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            tqdm_dict = {'d_loss': d_loss}\n",
        "            log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n",
        "            output = OrderedDict({\n",
        "                'loss': d_loss,\n",
        "                'progress_bar': tqdm_dict,\n",
        "                'log': log\n",
        "            })\n",
        "            return output\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_gen = self.hparams.lr_gen\n",
        "        lr_dis = self.hparams.lr_dis\n",
        "        b1 = self.hparams.b1\n",
        "        b2 = self.hparams.b2\n",
        "\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr_gen, betas=(b1, b2))\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr_dis, betas=(b1, b2))\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def backward(self, trainer, loss, optimizer, optimizer_idx: int) -> None:\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n",
        "        #                                 transforms.ToTensor(),\n",
        "        #                                 transforms.Normalize([0.5], [0.5])])\n",
        "        # dataset = torchvision.datasets.MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "        # return DataLoader(dataset, batch_size=self.hparams.batch_size)\n",
        "        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n",
        "        #                                 transforms.ToTensor(),\n",
        "        #                                 transforms.Normalize([0.5], [0.5])\n",
        "        #                                 ])\n",
        "        # train_dataset = torchvision.datasets.ImageFolder(\n",
        "        #     root=\"./drive/My Drive/datasets/flower_dataset/\",\n",
        "        #     # root=\"./drive/My Drive/datasets/ghibli_dataset_small_overfit/\",\n",
        "        #     transform=transform\n",
        "        # )\n",
        "        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True,\n",
        "        #                   batch_size=self.hparams.batch_size)\n",
        "        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.5], [0.5])\n",
        "                                        ])\n",
        "\n",
        "        train_dataset = torchvision.datasets.ImageFolder(\n",
        "            root=\"./drive/My Drive/datasets/ghibli_dataset_small_overfit/\",\n",
        "            transform=transform\n",
        "        )\n",
        "        return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True,\n",
        "                          batch_size=self.hparams.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        z = torch.randn(4, self.hparams.latent_dim, 1, 1).cuda()\n",
        "        # match gpu device (or keep as cpu)\n",
        "        if self.on_gpu:\n",
        "            z = z.cuda(self.last_imgs.device.index)\n",
        "\n",
        "        # log sampled images\n",
        "        sample_imgs = self.generator(z)[0]\n",
        "        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n",
        "        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n",
        "        grid = grid.permute(1, 2, 0)\n",
        "        torchvision.utils.save_image(grid, f'generated_images_epoch{self.current_epoch}.png')\n",
        "\n",
        "        # save model\n",
        "        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n",
        "            trainer.save_checkpoint(\n",
        "                self.checkpoint_folder + \"/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpLirisygB7J"
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = {\n",
        "    'batch_size': 128, # batch size\n",
        "    'lr_gen': 0.0003,  # TTUR;learnin rate of both networks; tested value: 0.0002\n",
        "    'lr_dis': 0.0003,  # TTUR;learnin rate of both networks; tested value: 0.0002\n",
        "    'b1': 0.5,  # Momentum for adam; tested value(dcgan paper): 0.5\n",
        "    'b2': 0.999,  # Momentum for adam; tested value(dcgan paper): 0.999\n",
        "    'latent_dim': 256,  # tested value which worked(in V4_1): 100\n",
        "    'nc': 3,  # number of color channels\n",
        "    'ndf': 32,  # number of discriminator features\n",
        "    'ngf': 32,  # number of generator features\n",
        "    'epochs': 4,  # the maxima lamount of epochs the algorith should run\n",
        "    'save_model_every_epoch': 1,  # how often we save our model\n",
        "    'image_size': 128, # size of the image\n",
        "    'num_workers': 3,\n",
        "    'level_of_noise': 0.1,  # how much instance noise we introduce(std; tested value: 0.15 and 0.1\n",
        "    'experience_save_per_batch': 1,  # this value should be very low; tested value which works: 1\n",
        "    'experience_batch_size': 50  # this value shouldnt be too high; tested value which works: 50\n",
        "}\n",
        "hparams = Namespace(**args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiLF4R3gDc1"
      },
      "source": [
        "# Parameters\n",
        "experiment_name = \"DCGAN_6_2_MNIST_128px\"\n",
        "dataset_name = \"mnist\"\n",
        "checkpoint_folder = \"DCGAN/\"\n",
        "tags = [\"DCGAN\", \"128x128\"]\n",
        "dirpath = Path(checkpoint_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWAfRXsVgGTX"
      },
      "source": [
        "# defining net\n",
        "net = DCGAN(hparams, checkpoint_folder, experiment_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D_VasOmgJ1N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc0c05a05ff84229a6ab8eb32783e09f",
            "465a6f8aff6a4924aff55f808536a0b8",
            "c88273c3c0af4485aa796ecacfc5c21d",
            "cca4bbb1f9bb49f082c74edb50c29fc9",
            "e2b295e333c848ccb33feb8bda6cd7a2",
            "524796e3ee2b47e881d6c435a244a727",
            "c29b6f0a22ed41809d110459ef585b61",
            "4a58e2fccb6e49c193caea3f88236bc9"
          ]
        },
        "outputId": "6b2f91bb-ec5c-4362-e3b5-aaf2ae38ca29"
      },
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "trainer = pl.Trainer( # resume_from_checkpoint=\"DCGAN_V4_2_GHIBLI_epoch_999.ckpt\",\n",
        "    max_epochs=args[\"epochs\"],\n",
        "    gpus=1\n",
        ")\n",
        "\n",
        "trainer.fit(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 262), '|u1')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7acdc11c3451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_core_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# model hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_function_implemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_epoch_end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e3cfa86ba50d>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'generated_images_epoch{self.current_epoch}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, nrow, padding, normalize, range, scale_each, pad_value, format)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2680\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 262), |u1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE7Q9vDQfGn6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}